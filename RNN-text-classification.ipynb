{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_docs(raw_docs):\n",
    "    # 用來存放分詞後的結果\n",
    "    docs = []\n",
    "    # stopword\n",
    "    with open(\"data/jieba_dict/stopwords.txt\") as stop_words:\n",
    "        stop_word_list = [stop_word.strip() for stop_word in stop_words]\n",
    "    # 支援繁體中文較好的詞庫\n",
    "    jieba.set_dictionary(\"data/jieba_dict/dict.txt.big\")\n",
    "    jieba.load_userdict(\"data/jieba_dict/中央機構.dict\")\n",
    "    jieba.load_userdict(\"data/jieba_dict/名人錄.dict\")\n",
    "    jieba.load_userdict(\"data/jieba_dict/專有名詞.dict\")\n",
    "    jieba.load_userdict(\"data/jieba_dict/縣市區鄉鎮.dict\")\n",
    "\n",
    "    for index, doc in enumerate(raw_docs, 0):\n",
    "        if index % 2000 == 0:\n",
    "            print(\"current document index:{}\".format(index))\n",
    "        # 去掉非英文中文數字    \n",
    "        doc = filter_regx_word(doc)\n",
    "        # 分詞\n",
    "        doc = jieba.cut(doc)\n",
    "        # 去掉保留字\n",
    "        doc = list(filter(lambda x: x not in stop_word_list, doc))\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_regx_word(document):\n",
    "# 只取中文\n",
    "    try:\n",
    "        document = \"\".join(re.findall(r\"[\\u4e00-\\u9fa5]\", document))\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        print(\"{}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_record_by_label(raw_df, num):\n",
    "    temp_df = pd.DataFrame()\n",
    "    raw_df = shuffle(raw_df)\n",
    "    labels = raw_df.groupby('label').size().index.values\n",
    "    for label in labels:\n",
    "        temp_df = temp_df.append(raw_df.loc[raw_df[\"label\"]==label,:].iloc[:num])  \n",
    "    return shuffle(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_network_model(vocab_size, max_text_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_text_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入不同主題資料\n",
    "topic_list = [\"政治\", \"科技\", \"娛樂\", \"體育\", \"社會\", \"財經\", \"健康\", \"國際\"]\n",
    "raw_df = pd.DataFrame()\n",
    "\n",
    "for index, topic in enumerate(topic_list, 0):\n",
    "    with open(\"data/text/big_data/corpus/\" + topic + \".txt\", \"r\", encoding=\"utf-8\") as content:\n",
    "        content_list = [line for line in content]\n",
    "    temp_df = pd.DataFrame(content_list, columns=['content'])\n",
    "    temp_df['label'] = index\n",
    "    raw_df = raw_df.append(temp_df)\n",
    "\n",
    "raw_df = sample_record_by_label(raw_df, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_df['content'].values\n",
    "y = raw_df['label'].values\n",
    "# 切分訓練與測試\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/mark/Documents/python/nlp-experiment/data/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.uf13363f31a3360411b43fe8e84af1634.cache\n",
      "Loading model cost 1.414 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current document index:0\n",
      "current document index:2000\n",
      "current document index:4000\n"
     ]
    }
   ],
   "source": [
    "docs = preprocess_raw_docs(X_train[:6000])\n",
    "docs = [\" \".join(doc) for doc in docs]\n",
    "labels = y_train[:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 200\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model memory too big!\n",
    "# pretrain_model = Word2Vec.load(\"data/model/Word2Vec_v1.4/w2v.model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 200, 100)          11616500  \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 160008    \n",
      "=================================================================\n",
      "Total params: 11,776,508\n",
      "Trainable params: 11,776,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = construct_network_model(vocab_size, max_length)\n",
    "model.summary()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4800 samples, validate on 1200 samples\n",
      "Epoch 1/10\n",
      "4800/4800 [==============================] - 2s - loss: 1.7366 - acc: 0.4133 - val_loss: 1.1145 - val_acc: 0.6925\n",
      "Epoch 2/10\n",
      "4800/4800 [==============================] - 2s - loss: 0.3931 - acc: 0.9396 - val_loss: 0.7039 - val_acc: 0.7875\n",
      "Epoch 3/10\n",
      "4800/4800 [==============================] - 2s - loss: 0.0756 - acc: 0.9910 - val_loss: 0.6394 - val_acc: 0.7892\n",
      "Epoch 4/10\n",
      "4800/4800 [==============================] - 2s - loss: 0.0389 - acc: 0.9946 - val_loss: 0.6273 - val_acc: 0.7867\n",
      "Epoch 5/10\n",
      "4800/4800 [==============================] - 2s - loss: 0.0232 - acc: 0.9965 - val_loss: 0.6206 - val_acc: 0.7917\n",
      "Epoch 6/10\n",
      "4800/4800 [==============================] - 2s - loss: 0.0179 - acc: 0.9971 - val_loss: 0.6145 - val_acc: 0.7900\n",
      "Epoch 7/10\n",
      "4800/4800 [==============================] - 2s - loss: 0.0144 - acc: 0.9967 - val_loss: 0.6150 - val_acc: 0.7908\n",
      "Epoch 8/10\n",
      "4800/4800 [==============================] - 2s - loss: 0.0116 - acc: 0.9975 - val_loss: 0.6207 - val_acc: 0.7900\n",
      "Epoch 9/10\n",
      "4800/4800 [==============================] - 2s - loss: 0.0104 - acc: 0.9975 - val_loss: 0.6149 - val_acc: 0.7883\n",
      "Epoch 10/10\n",
      "4800/4800 [==============================] - 2s - loss: 0.0099 - acc: 0.9975 - val_loss: 0.6192 - val_acc: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fabef1fd208>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_docs, labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/mark/Documents/python/nlp-experiment/data/jieba_dict/dict.txt.big ...\n",
      "Loading model from cache /tmp/jieba.uf13363f31a3360411b43fe8e84af1634.cache\n",
      "Loading model cost 1.328 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current document index:0\n"
     ]
    }
   ],
   "source": [
    "test_docs = preprocess_raw_docs(X_test[:10])\n",
    "test_docs = [\" \".join(doc) for doc in test_docs]\n",
    "test_docs = pad_sequences(t.texts_to_sequences(test_docs), maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 5, 0, 6, 4, 0, 3, 3, 2, 5])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 0, 6, 4, 0, 3, 3, 4, 5])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
